{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\enriroma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\ENRIROMA\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for os\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa7a13e-b5ba-470a-9120-dd5e83e8e786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0acb0a7c-edcf-4d8e-9f70-5d24378ef2be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_data_from_blob_storage(folder_name, blob_name, connection_string):\n",
    "    \"\"\"\n",
    "    Liest eine Datei aus einem bestimmten Ordner im Azure Blob Storage und gibt den Inhalt als DataFrame zurück.\n",
    "\n",
    "    Args:\n",
    "        folder_name (str): Name des Ordners im Blob Storage.\n",
    "        blob_name (str): Name der Datei im Blob Storage.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Inhalt der Datei als DataFrame.\n",
    "    \"\"\"\n",
    "    # Establish connection to the Azure Blob Storage account\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "    # Access the container\n",
    "    container_name = \"source-data\"\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # Build the full path to the blob within the folder\n",
    "    blob_path = f\"{folder_name}/{blob_name}\"\n",
    "\n",
    "    # Download the file\n",
    "    blob_client = container_client.get_blob_client(blob_path)\n",
    "\n",
    "    # Use a temporary file to download the blob\n",
    "    temp_file_path = blob_name \n",
    "    with open(temp_file_path, \"wb\") as download_file:\n",
    "        download_file.write(blob_client.download_blob().readall())\n",
    "\n",
    "    # Load the file into a DataFrame depending on its extension\n",
    "    if temp_file_path.lower().endswith(\".parquet\"):\n",
    "        df = pd.read_parquet(temp_file_path)\n",
    "    elif temp_file_path.lower().endswith(\".csv\"):\n",
    "        df = pd.read_csv(temp_file_path, sep=\";\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Only .parquet and .csv are supported.\")\n",
    "\n",
    "    # Remove the temporary file if needed\n",
    "    os.remove(temp_file_path)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    df.rename(columns={\"datum\":\"datetime\",\"Überschuss\":\"feed_in:kWh\", \"Produktion\": \"production:kWh\", \"Eigenverbrauch\": \"self-consumption:kWh\",\n",
    "                        \"PLZ\":\"zip_code\", \"Ort\":\"city\", \"PanelPeakLeistung\": \"panel_peak_power:kWp\", \"Ausrichtung\": \"orientation\", \"Anstellwinkel\": \"tilt:deg\",\n",
    "                        \"Ausrichtung_Grad\": \"orientation:deg\",  'Installierte, nominale Speicherkapazität (kWh)': \"battery_capacity:kWh\",\n",
    "                        \"Kategorie\":\"category\"},inplace=True)\n",
    "    df.drop(columns=[\"date\"],inplace=True)\n",
    "    return df\n",
    "\n",
    "def extend_with_alternator(df):\n",
    "    # Neuer Wert für PanelPeakLeistung, nur für ID 8\n",
    "    new_panel_peak_value = 700\n",
    "\n",
    "    # Aktualisieren der PanelPeakLeistung für die ID 8\n",
    "    df.loc[df['id'] == 8, 'panel_peak_power:kWp'] = new_panel_peak_value\n",
    "\n",
    "    new_data_ac = {\n",
    "    'id': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],\n",
    "    'ac_alternator_power:kVA': [206,82,70,43,100,36,200,450,320,55,58.3,940,204,79,25,200,235,77.6,50,50,470,66.6,30,72.5,40,52,60,200,220,150]\n",
    "    }\n",
    "\n",
    "    new_df_ac = pd.DataFrame(new_data_ac)\n",
    "\n",
    "    # Hinzufügen der neuen Spalte zum DataFrame\n",
    "    df = df.merge(new_df_ac, on='id', how='left')\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363cc799-a2cb-486b-a80d-523c6a239787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connection_string = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175dfcdc-8c44-4380-8ec8-ff01039ff43f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_PV_small = get_data_from_blob_storage(\"Innovation_Days\", \"data_PV_smaller_30.parquet\", connection_string)\n",
    "df_PV_small = prepare_data(df_PV_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PV_large = get_data_from_blob_storage(\"Innovation_Days\", \"data_PV_bigger_30.parquet\", connection_string)\n",
    "df_PV_large = prepare_data(df_PV_large)\n",
    "df_PV_large = extend_with_alternator(df_PV_large)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RetreivePV_Data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
